{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "source_file='data_thchs30'\n",
    "def source_get(source_file):\n",
    "    train_file='./'+source_file+'/data'\n",
    "    label_lst=[]\n",
    "    wav_lst=[]\n",
    "    for root, dirs, files in os.walk(train_file):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav') or file.endswith('.WAV'):\n",
    "                wav_file=os.sep.join([root,file])\n",
    "                label_file=wav_file+'.trn'\n",
    "                wav_lst.append(wav_file)\n",
    "                label_lst.append(label_file)\n",
    "    return label_lst, wav_lst\n",
    "# label_lst 是类标文件，即.trn为后缀的文件\n",
    "# wav_lst 是语音文件，即.wav是后缀的文件\n",
    "# 这两个文件是训练数据\n",
    "label_lst, wav_lst=source_get(source_file)\n",
    "print('length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行验证数据读取过程中有没有问题\n",
    "for i in range(len(label_lst)):\n",
    "    wavname=(wav_lst[i].split('/')[-1]).split('.')[0]\n",
    "    labelname = (label_lst[i].split('/')[-1]).split('.')[0]\n",
    "    if wavname != labelname:\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label(label_file):\n",
    "    with open(label_file, 'r', encoding='utf8') as f:\n",
    "        data=f.readlines()\n",
    "        return data[1]\n",
    "#label_file当中第一行存储的是中文字符，第二行存储的是对应的语音，第三行是对应的语素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ta1 jin3 ping2 yao1 bu4 de5 li4 liang4 zai4 yong3 dao4 shang4 xia4 fan1 teng2 yong3 dong4 she2 xing2 zhuang4 ru2 hai3 tun2 yi4 zhi2 yi3 yi4 tou2 de5 you1 shi4 ling3 xian1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 总共有13388条语音，对应着13388条拼音记录\n",
    "# gen_label_data函数返回这13388条语音记录对应拼音\n",
    "def gen_label_data(label_lst):\n",
    "    label_data=[]\n",
    "    for label_file in label_lst:\n",
    "        pny=read_label(label_file)\n",
    "        label_data.append(pny.strip('\\n'))\n",
    "    return label_data\n",
    "\n",
    "label_data=gen_label_data(label_lst)\n",
    "label_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过统计语料获得有多少发音，并把这些发音存储到vocab中\n",
    "# vocab就是一个字典的功能，汉语拼音和对应的索引构成字典\n",
    "def mk_vocab(label_data):\n",
    "    vocab=[]\n",
    "    for line in label_data:\n",
    "        line=line.split(' ')\n",
    "        for pny in line:\n",
    "            if pny not in vocab:\n",
    "                vocab.append(pny)\n",
    "    vocab.append('_')\n",
    "    return vocab\n",
    "\n",
    "vocab =mk_vocab(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于vocab字典获取label（拼音）对应的id\n",
    "def word2id(line, vocab):\n",
    "    return [vocab.index(pny) for pny in line.split(' ')]#以空格为分隔符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209\n"
     ]
    }
   ],
   "source": [
    "total_nums = 100\n",
    "batch_size = 20\n",
    "batch_num = total_nums // batch_size\n",
    "epochs = 50\n",
    "\n",
    "print(len(vocab))\n",
    "shuffle_list = [i for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打乱数据的顺序，我们通过查询乱序的索引，确定训练数据的顺序\n",
    "from random import shuffle\n",
    "shuffle_list=[i for i in range(total_nums)]\n",
    "shuffle(shuffle_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_padding(wav_data_lst):\n",
    "    wav_lens = [data.shape[0] for data in wav_data_lst]\n",
    "    wav_max_len = max(wav_lens)\n",
    "    wav_lens = np.array([leng//8 for leng in wav_lens])\n",
    "    new_wav_data_lst = np.zeros((len(wav_data_lst), wav_max_len, 120,  1))\n",
    "    for i in range(len(wav_data_lst)):\n",
    "        new_wav_data_lst[i, :wav_data_lst[i].shape[0], :, 0] = wav_data_lst[i]\n",
    "    return new_wav_data_lst, wav_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_padding(label_data_lst):\n",
    "    label_lens = np.array([len(label) for label in label_data_lst])\n",
    "    max_label_len = max(label_lens)\n",
    "    new_label_data_lst = np.zeros((len(label_data_lst), max_label_len))\n",
    "    for i in range(len(label_data_lst)):\n",
    "        new_label_data_lst[i][:len(label_data_lst[i])] = label_data_lst[i]\n",
    "    return new_label_data_lst,label_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def data_generator(batch_size, shuffle_list, wav_lst, label_data, vocab):\n",
    "    for i in range(len(wav_lst)//batch_size):\n",
    "        wav_data_lst=[]\n",
    "        label_data_lst=[]\n",
    "        begin=i*batch_size\n",
    "        end=begin+batch_size\n",
    "        sub_list=shuffle_list[begin:end]\n",
    "        for index in sub_list:\n",
    "            x,r=librosa.load(wav_lst[index])\n",
    "            mfcc=librosa.feature.mfcc(x,r,n_mfcc=120).T\n",
    "            pad_mfcc=np.zeros((mfcc.shape[0]//8*8+8,mfcc.shape[1]))\n",
    "            pad_mfcc[:mfcc.shape[0],:]=mfcc\n",
    "            label=word2id(label_data[index],vocab)\n",
    "            wav_data_lst.append(pad_mfcc)\n",
    "            label_data_lst.append(label)\n",
    "        pad_wav_data,input_length=wav_padding(wav_data_lst)\n",
    "        print(pad_wav_data.shape)\n",
    "        pad_label_data,label_length=label_padding(label_data_lst)\n",
    "        inputs = {'the_inputs': pad_wav_data,\n",
    "                  'the_labels': pad_label_data,\n",
    "                  'input_length': input_length,\n",
    "                  'label_length': label_length,\n",
    "                 }\n",
    "        outputs = {'ctc': np.zeros(pad_wav_data.shape[0],)} \n",
    "#         return inputs, outputs\n",
    "        yield inputs, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336, 120)\n",
      "(344, 120)\n",
      "[-4.46572715e+02  9.26332211e+01  9.99465923e-01  2.03506718e+01\n",
      "  2.65053779e+01  5.88762892e+00  3.02444206e+00  3.34910065e+00\n",
      "  1.59525124e+01  2.29271629e+01  9.26783102e+00  7.56682440e+00\n",
      "  1.22366358e+01  9.83432997e+00  1.33641370e+01  1.25091871e+01\n",
      "  3.59305888e+00  1.81684241e+00  4.09812087e+00  4.41741694e+00\n",
      "  2.08384104e+00 -1.58594964e+00  1.41728099e+00  6.32762927e+00\n",
      "  4.87731823e+00  1.17850904e+00 -2.57193895e+00 -3.01324504e-01\n",
      "  4.61401160e+00 -2.47471393e+00 -5.61845880e+00  3.66761102e+00\n",
      "  9.51778869e-01 -5.19791760e+00  1.35770385e+00  2.50211155e+00\n",
      " -9.75789262e-01  1.46323353e+00 -4.33361416e-01 -3.72260400e+00\n",
      " -1.51385918e+00 -1.48119765e+00 -4.15058199e-01  6.31708049e+00\n",
      "  7.39308611e+00 -3.36807754e-01 -4.05350348e+00  2.41326216e+00\n",
      "  1.00836643e+01  7.47808430e+00 -2.07632364e+00 -3.89542263e+00\n",
      "  5.02759406e-01  1.32004771e-01  3.14348593e+00  1.10525731e+01\n",
      "  7.42853101e+00  8.25944422e-02  3.14807913e+00  3.93206427e+00\n",
      " -7.19935651e-01 -6.69980431e-01 -7.31679648e-01 -3.28131260e+00\n",
      " -1.22201658e+00  4.31352975e+00  5.95098430e+00  3.65657587e+00\n",
      "  3.70882187e+00  2.82658702e+00 -2.11716954e+00 -4.04664854e+00\n",
      " -3.63821638e+00 -1.85708393e+00  4.42148026e-01 -4.74290799e+00\n",
      " -7.73320591e+00 -1.52083627e+00  8.67780916e-01  3.83241663e-01\n",
      "  2.08567185e+00  4.21708018e+00  7.37616978e+00  4.18739401e+00\n",
      " -4.18946092e+00 -4.66538029e+00  2.43579078e+00  6.35173286e+00\n",
      "  2.21069128e+00 -2.30916581e+00 -2.64367174e+00 -2.85884847e+00\n",
      " -1.31957343e+00  3.44239727e-01  2.06406786e-01  1.20094462e+00\n",
      " -6.57414834e-02 -7.83168291e-01  1.30651170e+00  1.51257759e-01\n",
      "  3.12261099e-02  1.86215793e+00  1.35249874e+00  1.71578607e+00\n",
      " -8.58068919e-02 -2.72830372e+00  7.23991556e-01  2.60374481e+00\n",
      "  3.61254096e-01  2.02593537e+00  3.16543430e+00 -8.69109180e-01\n",
      " -2.41380356e+00 -6.97447338e-01 -2.52256646e+00 -4.29795770e+00\n",
      " -2.05968868e+00 -4.66363233e-01  1.68007769e+00  4.37740439e+00]\n"
     ]
    }
   ],
   "source": [
    "x,r=librosa.load(wav_lst[0])\n",
    "mfcc=librosa.feature.mfcc(x,r,n_mfcc=120).T\n",
    "print(mfcc.shape)\n",
    "# 进行了三次卷积核pooling因此数据大小改为原有的1/8。所以数据的长宽需要被8整除\n",
    "pad_mfcc=np.zeros((mfcc.shape[0]//8*8+8,mfcc.shape[1]))\n",
    "print(pad_mfcc.shape)\n",
    "pad_mfcc[:mfcc.shape[0],:]=mfcc\n",
    "print(pad_mfcc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 512, 120, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, outputs=next(data_generator(batch_size, shuffle_list, wav_lst, label_data, vocab))\n",
    "# 这个ctc.shape[0]实际上是batch大小，这个输出结果实际上是(batch_size,None)。这里None是因为每一条训练语音可采样不同长度个帧\n",
    "# 因此对应不同长度的类标\n",
    "outputs['ctc'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8c47e9746b11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D\n",
    "from keras.layers import Reshape, Dense, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(size):\n",
    "    return Conv2D(size, (3,3), use_bias=True, activation='relu',\n",
    "        padding='same', kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return BatchNormalization(axis=-1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool(x):\n",
    "    return MaxPooling2D(pool_size=(2,2), strides=None, padding=\"valid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(units, activation=\"relu\"):\n",
    "    return Dense(units, activation=activation, use_bias=True,\n",
    "        kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape=(none, none, none)\n",
    "# output.shape = (1/2, 1/2, 1/2)\n",
    "def cnn_cell(size, x, pool=True):\n",
    "    x = norm(conv2d(size)(x))\n",
    "    x = norm(conv2d(size)(x))\n",
    "    if pool:\n",
    "        x = maxpool(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args: self.labels, self.outputs, self.input_length, self.label_length\n",
    "def ctc_lambda(args):\n",
    "    labels, y_pred, input_length, label_length = args\n",
    "    y_pred = y_pred[:, :, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Amodel():\n",
    "    \"\"\"docstring for Amodel.\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Amodel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self._model_init()\n",
    "        self._ctc_init()\n",
    "        self.opt_init()\n",
    "\n",
    "    def _model_init(self):\n",
    "        self.inputs = Input(name='the_inputs', shape=(None, 120, 1))\n",
    "        self.h1 = cnn_cell(32, self.inputs)\n",
    "        self.h2 = cnn_cell(64, self.h1)\n",
    "        self.h3 = cnn_cell(128, self.h2)\n",
    "        self.h4 = cnn_cell(128, self.h3, pool=False)\n",
    "        # 120 / 8 * 128 = 1920\n",
    "        self.h6 = Reshape((-1, 1920))(self.h4)\n",
    "        self.h7 = dense(256)(self.h6)\n",
    "        self.outputs = dense(self.vocab_size, activation='softmax')(self.h7)\n",
    "#         self.model = Model(inputs=self.inputs, outputs=self.outputs)\n",
    "\n",
    "    def _ctc_init(self):\n",
    "        self.labels = Input(name='the_labels', shape=[None], dtype='float32')\n",
    "        self.input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "        self.label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "#         当要使用一些keras.layers中没有的层的时候，需想办法将其转换成keras中的Layer。\n",
    "#         其中Lambda函数就是一种方法。定义一个squeeze层，其中x是上一层的输出，\n",
    "#         Lambda(keras.backend.squeeze)(x)\n",
    "        self.loss_out = Lambda(ctc_lambda, output_shape=(1,), name='ctc')\\\n",
    "            ([self.labels, self.outputs, self.input_length, self.label_length])\n",
    "        self.ctc_model = Model(inputs=[self.labels, self.inputs,\n",
    "            self.input_length, self.label_length], outputs=self.loss_out)\n",
    "        \n",
    "    def opt_init(self):\n",
    "        opt = Adam(lr = 0.0008, beta_1 = 0.9, beta_2 = 0.999, decay = 0.01, epsilon = 10e-8)\n",
    "        #self.ctc_model=multi_gpu_model(self.ctc_model,gpus=2)\n",
    "        self.ctc_model.compile(loss={'ctc': lambda y_true, output: output}, optimizer=opt)\n",
    "# loss: 字符串（目标函数名）或目标函数或 Loss 实例。 如果模型具有多个输出，\n",
    "# 则可以通过传递损失函数的字典或列表，在每个输出上使用不同的损失。\n",
    "# 模型将最小化的损失值将是所有单个损失的总和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am=Amodel(1176)\n",
    "am.ctc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nums = 100\n",
    "batch_size = 20\n",
    "batch_num = total_nums // batch_size\n",
    "epochs = 50\n",
    "source_file = 'data_thchs30'\n",
    "label_lst, wav_lst = source_get(source_file)\n",
    "label_data = gen_label_data(label_lst[:100])\n",
    "vocab = mk_vocab(label_data)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab_size)\n",
    "\n",
    "shuffle_list = [i for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am = Amodel(vocab_size)\n",
    "\n",
    "for k in range(epochs):\n",
    "    print('this is the', k+1, 'th epochs trainning !!!')\n",
    "    #shuffle(shuffle_list)\n",
    "    batch = data_generator(batch_size, shuffle_list, wav_lst, label_data, vocab)\n",
    "    am.ctc_model.fit_generator(batch, steps_per_epoch=batch_num, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 548,
   "position": {
    "height": "40px",
    "left": "803px",
    "right": "20px",
    "top": "77px",
    "width": "626px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
